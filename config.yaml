# config.yaml

model:
  variant: "MAC"                 # Options: baseline, MAC, MAG, MAL, LMM
  d_model: 32
  n_heads: 2
  n_layers: 2                    # For baseline transformer
  vocab_size: 50
  max_seq_len: 128

  # Titans-specific configs
  memory_layers: 2               # Depth of neural memory (L_M)
  persistent_memory_size: 8      # Size of persistent memory (N_p)

  # Segmentation-specific configs - not using them in the PoC
  memory:
    window_size: 32              # I'd use this for setting up sliding window attention (MAG, MAL)
    segment_size: 32             # And this for setting up the MAC chunking

training:
  batch_size: 4
  learning_rate: 0.0001          # keeping it small to get a curve on the "toy" datasets
  weight_decay: 0.1
  max_epochs: 100
  gradient_clip: 1.0
  run_comparison: true          # Set to true if you want to run a comparison betweeen models

  # Optimizer
  optimizer: "adamw"
#  scheduler: "cosine"           # Uncomment if you want to use a scheduler

data:
  dataset: "needle_haystack"           # which dataset/task to use [copy_task, language_modeling, needle_haystack]
  seq_len: 64
  vocab_size: 10
  num_samples: 1000
  train_ratio: 0.8

# For reproducibility
seed: 42

# Logging
logging:
  log_level: "INFO"
  log_every: 10
#  save_every: 50                # Uncomment if you want to save checkpoints every N steps


# Paths
paths:
  data_dir: "./data"             # not using this atm as creating synthetic data, but downloaded data would go here.
  log_dir: "./logs"
  results_dir: "./results"